{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=(3,32,32), n_out=10):\n",
    "        super(Network, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((3,3))\n",
    "        )\n",
    "        self.flat_fts = self.get_flat_fts(input_size, self.features)\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, 100),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, 100),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,n_out),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def get_flat_fts(self, in_size, fts):\n",
    "        f = fts(Variable(torch.ones(1,*in_size)))\n",
    "        return int(np.prod(f.size()[1:]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fts = self.features(x)\n",
    "        flat_fts = fts.view(-1, self.flat_fts)\n",
    "        out1 = self.classifier1(flat_fts)\n",
    "        out2 = self.classifier2(flat_fts)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = {}\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "    return hook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x  = Variable(torch.randn(2,3,32,32), requires_grad=True)\n",
    "y1 = Variable(torch.rand(2,1), requires_grad=False)\n",
    "y2 = Variable(torch.randperm(10)[:2], requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network()\n",
    "net.zero_grad()\n",
    "\n",
    "out1, out2 = net(x)\n",
    "\n",
    "out1.register_hook(save_grad('out1'))\n",
    "out2.register_hook(save_grad('out2'))\n",
    "\n",
    "out1.size(), out2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fb1113ffc50>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion1 = nn.BCELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "\n",
    "loss1 = criterion1(out1, y1)\n",
    "loss2 = criterion2(out2, y2)\n",
    "\n",
    "loss1.register_hook(save_grad('loss1'))\n",
    "loss2.register_hook(save_grad('loss2'))\n",
    "\n",
    "lam = 0.5\n",
    "loss = loss1 + lam * loss2\n",
    "loss.register_hook(save_grad('loss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.6935\n",
       " [torch.FloatTensor of size 1], \n",
       "  2.2977\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1.data, loss2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5000\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['loss2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0251  0.0248  0.0246  0.0249  0.0257 -0.2247  0.0250  0.0250  0.0248  0.0250\n",
       " 0.0252  0.0249  0.0245  0.0248  0.0255  0.0251 -0.2250  0.0249  0.0250  0.0250\n",
       "[torch.FloatTensor of size 2x10]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads['out2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 595291 of gradients tuple is None, but the corresponding Variable requires grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-b68e87170168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_variables)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[1;32m     41\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 42\u001b[0;31m         tuple(variables), tuple(grad_variables), retain_variables)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autograd_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 595291 of gradients tuple is None, but the corresponding Variable requires grad"
     ]
    }
   ],
   "source": [
    "torch.autograd.backward(x, [out1.grad, out2.grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 10.0909\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grads = {}\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "    return hook\n",
    "\n",
    "x = Variable(torch.randn(1,1), requires_grad=True)\n",
    "y = 3*x\n",
    "z = y**2\n",
    "\n",
    "# In here, save_grad('y') returns a hook (a function) that keeps 'y' as name\n",
    "y.register_hook(save_grad('y'))\n",
    "z.register_hook(save_grad('z'))\n",
    "z.backward()\n",
    "\n",
    "print(grads['y'])\n",
    "print(grads['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
