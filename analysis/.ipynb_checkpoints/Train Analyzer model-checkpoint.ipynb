{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os, time\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# these magics ensure that external modules that are modified are also automatically reloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# widgets and interaction\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gzip\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../ml-model/\")\n",
    "\n",
    "import models.dcgan as dcgan\n",
    "import models.mlp as mlp\n",
    "import models.dcgan_orig as do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_list(s):\n",
    "    s = re.sub('\\s+', ' ', s)[2:-2]\n",
    "    return tuple([float(n.strip()) for n in s.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>country</th>\n",
       "      <th>population</th>\n",
       "      <th>city</th>\n",
       "      <th>phase</th>\n",
       "      <th>region</th>\n",
       "      <th>build pct 128</th>\n",
       "      <th>fractal dim 128</th>\n",
       "      <th>patch distr 128</th>\n",
       "      <th>build pct 448</th>\n",
       "      <th>fractal dim 448</th>\n",
       "      <th>patch distr 448</th>\n",
       "      <th>build pct 64</th>\n",
       "      <th>fractal dim 64</th>\n",
       "      <th>patch distr 64</th>\n",
       "      <th>decile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/data/world-cities//train/medium/SAR_in_i...</td>\n",
       "      <td>medium</td>\n",
       "      <td>in</td>\n",
       "      <td>100585</td>\n",
       "      <td>itarsi, in (pop 100.6k)</td>\n",
       "      <td>train</td>\n",
       "      <td>Asia</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>1.802166</td>\n",
       "      <td>(0.0, 0.69314718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>1.559993</td>\n",
       "      <td>(3.17805383, 1.79175947, 2.30258509, 1.0986122...</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>1.511785</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/data/world-cities//train/medium/SAR_co_s...</td>\n",
       "      <td>medium</td>\n",
       "      <td>co</td>\n",
       "      <td>126553</td>\n",
       "      <td>sogamoso, co (pop 126.6k)</td>\n",
       "      <td>train</td>\n",
       "      <td>Americas</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>1.409835</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>1.081987</td>\n",
       "      <td>(2.56494936, 1.60943791, 1.79175947, 0.0, 0.0,...</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.862936</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/data/world-cities//train/medium/SAR_us_l...</td>\n",
       "      <td>medium</td>\n",
       "      <td>us</td>\n",
       "      <td>140772</td>\n",
       "      <td>lakewood, us (pop 140.8k)</td>\n",
       "      <td>train</td>\n",
       "      <td>Americas</td>\n",
       "      <td>0.027446</td>\n",
       "      <td>1.422604</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.69314718, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0.027316</td>\n",
       "      <td>1.254302</td>\n",
       "      <td>(3.55534806, 2.63905733, 1.79175947, 1.3862943...</td>\n",
       "      <td>0.026681</td>\n",
       "      <td>1.322882</td>\n",
       "      <td>(0.0, 1.09861229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/data/world-cities//train/medium/SAR_br_t...</td>\n",
       "      <td>medium</td>\n",
       "      <td>br</td>\n",
       "      <td>251647</td>\n",
       "      <td>taubate, br (pop 251.6k)</td>\n",
       "      <td>train</td>\n",
       "      <td>Americas</td>\n",
       "      <td>0.038727</td>\n",
       "      <td>1.711233</td>\n",
       "      <td>(1.79175947, 0.69314718, 1.60943791, 0.0, 0.0,...</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>1.464830</td>\n",
       "      <td>(4.33073334, 3.49650756, 3.25809654, 2.9957322...</td>\n",
       "      <td>0.038245</td>\n",
       "      <td>1.561612</td>\n",
       "      <td>(1.09861229, 0.69314718, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/data/world-cities//train/medium/SAR_ve_m...</td>\n",
       "      <td>medium</td>\n",
       "      <td>ve</td>\n",
       "      <td>105511</td>\n",
       "      <td>mariara, ve (pop 105.5k)</td>\n",
       "      <td>train</td>\n",
       "      <td>Americas</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>1.629995</td>\n",
       "      <td>(1.38629436, 0.0, 1.38629436, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.031249</td>\n",
       "      <td>1.415175</td>\n",
       "      <td>(3.98898405, 3.04452244, 2.7080502, 2.19722458...</td>\n",
       "      <td>0.032483</td>\n",
       "      <td>1.485818</td>\n",
       "      <td>(0.69314718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           filename   class  \\\n",
       "0           0  /home/data/world-cities//train/medium/SAR_in_i...  medium   \n",
       "1           1  /home/data/world-cities//train/medium/SAR_co_s...  medium   \n",
       "2           2  /home/data/world-cities//train/medium/SAR_us_l...  medium   \n",
       "3           3  /home/data/world-cities//train/medium/SAR_br_t...  medium   \n",
       "4           4  /home/data/world-cities//train/medium/SAR_ve_m...  medium   \n",
       "\n",
       "  country  population                       city  phase    region  \\\n",
       "0      in      100585    itarsi, in (pop 100.6k)  train      Asia   \n",
       "1      co      126553  sogamoso, co (pop 126.6k)  train  Americas   \n",
       "2      us      140772  lakewood, us (pop 140.8k)  train  Americas   \n",
       "3      br      251647   taubate, br (pop 251.6k)  train  Americas   \n",
       "4      ve      105511   mariara, ve (pop 105.5k)  train  Americas   \n",
       "\n",
       "   build pct 128  fractal dim 128  \\\n",
       "0       0.011440         1.802166   \n",
       "1       0.003207         1.409835   \n",
       "2       0.027446         1.422604   \n",
       "3       0.038727         1.711233   \n",
       "4       0.030704         1.629995   \n",
       "\n",
       "                                     patch distr 128  build pct 448  \\\n",
       "0  (0.0, 0.69314718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...       0.011681   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       0.003182   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.69314718, 0.0, 0.0, 0.0...       0.027316   \n",
       "3  (1.79175947, 0.69314718, 1.60943791, 0.0, 0.0,...       0.038346   \n",
       "4  (1.38629436, 0.0, 1.38629436, 0.0, 0.0, 0.0, 0...       0.031249   \n",
       "\n",
       "   fractal dim 448                                    patch distr 448  \\\n",
       "0         1.559993  (3.17805383, 1.79175947, 2.30258509, 1.0986122...   \n",
       "1         1.081987  (2.56494936, 1.60943791, 1.79175947, 0.0, 0.0,...   \n",
       "2         1.254302  (3.55534806, 2.63905733, 1.79175947, 1.3862943...   \n",
       "3         1.464830  (4.33073334, 3.49650756, 3.25809654, 2.9957322...   \n",
       "4         1.415175  (3.98898405, 3.04452244, 2.7080502, 2.19722458...   \n",
       "\n",
       "   build pct 64  fractal dim 64  \\\n",
       "0      0.012975        1.511785   \n",
       "1      0.002289        0.862936   \n",
       "2      0.026681        1.322882   \n",
       "3      0.038245        1.561612   \n",
       "4      0.032483        1.485818   \n",
       "\n",
       "                                      patch distr 64  decile  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       3  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       2  \n",
       "2  (0.0, 1.09861229, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...       5  \n",
       "3  (1.09861229, 0.69314718, 0.0, 0.0, 0.0, 0.0, 0...       6  \n",
       "4  (0.69314718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...       6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_df = pd.read_csv(\"/home/data/world-cities/urban_areas_over_10kpop_stats.csv\")\n",
    "files_df['patch distr 448'] = files_df['patch distr 448'].apply(parse_list)\n",
    "files_df['patch distr 128'] = files_df['patch distr 128'].apply(parse_list)\n",
    "files_df['patch distr 64'] = files_df['patch distr 64'].apply(parse_list)\n",
    "\n",
    "files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19988 4998\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(range(len(files_df)), size=int(0.8*len(files_df)), replace=False)\n",
    "train_df = files_df.iloc[idx]\n",
    "valid_df = files_df.iloc[list(set(range(len(files_df)))-set(idx))]\n",
    "\n",
    "print len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"/home/data/world-cities/\" + \"train.csv\")\n",
    "valid_df.to_csv(\"/home/data/world-cities/\" + \"valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pxCrop    = 448\n",
    "classCol  = \"patch distr 128\"\n",
    "imageSize = 128\n",
    "batchSize = 64\n",
    "workers   = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../ml-model/pytorch_utils\")\n",
    "from loader_dataframe import ImageDataFrame, grayscale_loader, WeightedRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def fn_rotate(img, max_angle=30):\n",
    "    theta = np.random.randn()*max_angle\n",
    "    return img.rotate(theta, expand=True)\n",
    "\n",
    "train_dataset = ImageDataFrame(df=train_df, \n",
    "                         classCol=classCol,\n",
    "                         loader=grayscale_loader,\n",
    "                         transform=transforms.Compose([\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.CenterCrop(pxCrop),\n",
    "                               transforms.Lambda(lambda img: fn_rotate(img)),\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0, 0, 0), (255.0, 255.0, 255.0))\n",
    "                           ]))\n",
    "valid_dataset = ImageDataFrame(df=valid_df, \n",
    "                         classCol=classCol,\n",
    "                         loader=grayscale_loader,\n",
    "                         transform=transforms.Compose([\n",
    "                               transforms.CenterCrop(pxCrop),\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0, 0, 0), (255.0, 255.0, 255.0))\n",
    "\n",
    "                           ]))\n",
    "\n",
    "list_output = train_df[classCol].dtype == object\n",
    "if list_output:\n",
    "    n_classes = len(train_df[classCol].iloc[0])\n",
    "else:\n",
    "    n_classes = len(train_dataset.classes) if train_dataset.classes is not None else None\n",
    "\n",
    "weights = None #train_df['built pct']\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, \n",
    "                                         #sampler=WeightedRandomSampler(weights, len(train_df)),\n",
    "                                         num_workers=int(workers))\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batchSize,\n",
    "                                         shuffle=False, \n",
    "                                         num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n",
    "- should work on arbitrary size images (multiple of 16)\n",
    "- dual output: 0/1 for fake/real, vector of stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nz = 100\n",
    "nc = 1\n",
    "ndf= 16\n",
    "ngpu = 4\n",
    "lr = 0.00005\n",
    "n_extra_layers = 0\n",
    "beta1 = 0.5\n",
    "lam = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class DCGAN_D_DUAL(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ndf, ngpu, n_extra_layers=0, n_classes=None):\n",
    "        super(DCGAN_D_DUAL, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        features = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        features.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        features.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            features.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            features.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            features.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            features.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            features.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            features.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        features.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, cndf, 4, 1, 0, bias=False))\n",
    "        self.features = features\n",
    "        self.flat_fts = self.get_flat_fts((nc,isize,isize), self.features)\n",
    "\n",
    "        self.classifier_src = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, 1),\n",
    "#             nn.Dropout(p=0.2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(100,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        if n_classes is None:\n",
    "            n_classes = 1\n",
    "        self.classifier_cls = nn.Sequential(\n",
    "            nn.Linear(self.flat_fts, n_classes),\n",
    "#             nn.Dropout(p=0.2),\n",
    "             nn.ReLU(),\n",
    "#             nn.Linear(100,n_classes),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def get_flat_fts(self, in_size, fts):\n",
    "        f = fts(Variable(torch.ones(1,*in_size)))\n",
    "        return int(np.prod(f.size()[1:]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fts = self.features(x)\n",
    "        flat_fts = fts.view(-1, self.flat_fts)\n",
    "        out1 = self.classifier1(flat_fts)\n",
    "        out2 = self.classifier2(flat_fts)\n",
    "        return out1, out2\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            fts = nn.parallel.data_parallel(self.features, input, range(self.ngpu))\n",
    "            flat_fts = fts.view(-1, self.flat_fts)\n",
    "            out_src= nn.parallel.data_parallel(self.classifier_src, flat_fts, range(self.ngpu))\n",
    "            out_cls= nn.parallel.data_parallel(self.classifier_cls, flat_fts, range(self.ngpu))            \n",
    "        else: \n",
    "            fts = self.features(input)\n",
    "            out_src= self.classifier_src(fts)\n",
    "            out_cls= self.classifier_cls(fts)\n",
    "                \n",
    "        return out_src, out_cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = DCGAN_D_DUAL(imageSize, nz, nc, ndf, ngpu, n_extra_layers, n_classes=n_classes)\n",
    "net.apply(weights_init)\n",
    "    \n",
    "input = torch.FloatTensor(batchSize, nc, imageSize, imageSize)\n",
    "if n_classes is not None:\n",
    "    if not list_output:\n",
    "        label = torch.LongTensor(batchSize, 1)\n",
    "    else:\n",
    "        label = torch.LongTensor(batchSize, n_classes)\n",
    "else:\n",
    "    label = torch.FloatTensor(batchSize, nc, imageSize, imageSize)\n",
    "source_label = torch.FloatTensor(batchSize)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "    input = input.cuda()\n",
    "    label, source_label = label.cuda(), source_label.cuda()\n",
    "\n",
    "input = Variable(input)\n",
    "label = Variable(label, requires_grad=False)\n",
    "source_label = Variable(source_label, requires_grad=False)\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# optimization criteria\n",
    "criterion_bce = nn.BCELoss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "criterion_cls = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def compute_eval_performance(net, valid_dataloader):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for imgs, labs in valid_dataloader:\n",
    "#         imgs = Variable(imgs.cuda())\n",
    "#         labs = labs.cuda()\n",
    "#         out_src_test, out_sts_test = net(imgs)\n",
    "#         loss_src_test = criterion_bce(out_src_test, source_label)\n",
    "#         loss_sts_test = criterion_cls(out_sts_test, labs)\n",
    "#         _, predicted = torch.max(out_sts_test.data, 1)\n",
    "#         total += labs.size(0)\n",
    "#         correct += (predicted == labs).sum()\n",
    "    \n",
    "#     val_acc = correct / float(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul received an invalid combination of arguments - got (float), but expected one of:\n * (int value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mfloat\u001b[0m)\n * (torch.cuda.LongTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mfloat\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-475af5dcfd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mout_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_sts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_bce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_sts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_sts\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lam can be a vector of weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# loss = 0*loss_src + lam * loss_sts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_sts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mMulConstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/basic_ops.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mul received an invalid combination of arguments - got (float), but expected one of:\n * (int value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mfloat\u001b[0m)\n * (torch.cuda.LongTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mfloat\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "save_dir = \"/home/data/pytorch-workspace/analyzer/classifier/\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "train_loss_hist = []\n",
    "valid_loss_hist = []\n",
    "best_val_perf  = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Train for this epoch\n",
    "    # --------------------\n",
    "    \n",
    "    net.train(True)\n",
    "    train_loss_epoch = []\n",
    "    for i, (imgs, labs) in enumerate(train_dataloader):    \n",
    "        labs = torch.cat(labs, 1)\n",
    "        batch_size = imgs.size(0)\n",
    "        input.data.resize_(imgs.size()).copy_(imgs)\n",
    "        label.data.resize_(labs.size()).copy_(labs)\n",
    "        source_label.data.resize_(batch_size).fill_(1) # right now, all images are real\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        net.zero_grad()\n",
    "        out_src, out_sts = net(input)\n",
    "        loss_src = criterion_bce(out_src, source_label)\n",
    "        loss_sts = criterion_mse(out_sts * lam, label * lam) # lam can be a vector of weights\n",
    "        # loss = 0*loss_src + lam * loss_sts    \n",
    "        loss = loss_sts\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f (Source: %.4f, Stats: %.4f)' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, \n",
    "                     loss.data[0], loss_src.data[0], loss_sts.data[0]*lam))\n",
    "        train_loss_epoch.append((loss.data[0], loss_src.data[0], loss_sts.data[0]*lam))\n",
    "    \n",
    "    train_loss_epoch = np.array(train_loss_epoch).mean(0)\n",
    "    train_loss_hist.append(train_loss_epoch)\n",
    "    \n",
    "    # Test for this epoch\n",
    "    # -------------------\n",
    "\n",
    "    net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    avg_valid_loss = []\n",
    "    for imgs, labs in valid_dataloader:\n",
    "        imgs = Variable(imgs.cuda())\n",
    "        labs = Variable(labs.float().cuda())\n",
    "        src_labs = Variable(torch.FloatTensor(labs.data.size(0)).cuda(), requires_grad=False)\n",
    "        out_src_valid, out_sts_valid = net(imgs)\n",
    "        loss_src_valid = criterion_bce(out_src_valid, src_labs)\n",
    "        loss_sts_valid = criterion_mse(out_sts_valid * lam, labs * lam)\n",
    "        loss_valid = loss_sts # + loss_src\n",
    "        avg_valid_loss.append((loss_valid.data[0], loss_src_valid.data[0], loss_sts_valid.data[0]))   \n",
    "        \n",
    "    avg_valid_loss = np.array(avg_valid_loss).mean(0)\n",
    "    valid_loss_hist.append(avg_valid_loss)\n",
    "    \n",
    "    # Track performance\n",
    "    # -----------------\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # plot performance vs epoch\n",
    "    fig, ax = plt.subplots(1,2, figsize=(8,3))\n",
    "    ax[0].plot(range(len(train_loss_hist)), \n",
    "             [x[0] for x in train_loss_hist],label=\"loss (train)\")\n",
    "    ax[0].plot(range(len(train_loss_hist)), \n",
    "             [x[2] for x in train_loss_hist],label=\"stat loss (train)\")\n",
    "    ax[0].legend(loc=\"best\")\n",
    "    ax[1].plot(range(len(valid_loss_hist)), \n",
    "             [x[0] for x in valid_loss_hist],label=\"loss (valid)\")\n",
    "    ax[1].plot(range(len(valid_loss_hist)), \n",
    "             [x[2] for x in valid_loss_hist],label=\"stat loss (valid)\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    plt.suptitle(\"Analyzer model performance\")\n",
    "    plt.savefig(\"%s/training_progress.jpg\"%save_dir)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "#     # checkpoint best model\n",
    "#     if val_acc > best_val_acc:\n",
    "#         print \"checkpointing: validation accuracy improved from %.2f to %.2f\"%(best_val_acc, val_acc)\n",
    "#         torch.save(net.state_dict(), '{0}/net_epoch{1}_acc{2:.2f}.pth'.format(save_dir, epoch, val_acc))\n",
    "#         best_val_acc = val_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    3     1     0  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "[torch.cuda.LongTensor of size 64x19 (GPU 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(imread(\"%s/training_progress.jpg\"%save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub received an invalid combination of arguments - got (torch.cuda.LongTensor), but expected one of:\n * (float value)\n * (torch.cuda.FloatTensor other)\n * (float value, torch.cuda.FloatTensor other)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4f37c5a4b8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_sts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__isub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36msub\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36m_sub\u001b[0;34m(self, other, inplace)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/basic_ops.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sub received an invalid combination of arguments - got (torch.cuda.LongTensor), but expected one of:\n * (float value)\n * (torch.cuda.FloatTensor other)\n * (float value, torch.cuda.FloatTensor other)\n"
     ]
    }
   ],
   "source": [
    "out_sts - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
